import os
import streamlit as st
from PIL import Image
from PyPDF2 import PdfReader
from langchain.text_splitter import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, OpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
import platform


st.set_page_config(
    page_title="Agente RAG con OpenAI ğŸ’¬",
    page_icon="ğŸ¤–",
    layout="centered"
)


st.title("ğŸ¤– GeneraciÃ³n Aumentada por RecuperaciÃ³n (RAG)")
st.caption(f"VersiÃ³n de Python: {platform.python_version()}")
st.markdown("""
Este asistente utiliza **RAG (Retrieval-Augmented Generation)** para responder preguntas basadas en el contenido de un PDF.
Sube un documento, haz una pregunta, y deja que el modelo te dÃ© una respuesta contextualizada. ğŸ“˜âœ¨
""")


try:
    image = Image.open('6b6dd5a171abf33c000b1ddb83bb4fe2.jpg')
    st.image(image, width=350, caption="AnÃ¡lisis inteligente de documentos PDF")
except Exception as e:
    st.warning(f"âš ï¸ No se pudo cargar la imagen: {e}")


with st.sidebar:
    st.header("ğŸ’¡ Instrucciones")
    st.markdown("""
    1. ğŸ”‘ Ingresa tu **clave de OpenAI**.  
    2. ğŸ“„ Sube un archivo PDF.  
    3. ğŸ’¬ Escribe una pregunta sobre el contenido.  
    4. ğŸ¤¯ El modelo buscarÃ¡ dentro del texto y te responderÃ¡.

    ---
    **Consejos:**
    - Usa documentos no muy extensos (mÃ¡x. ~50 pÃ¡ginas).  
    - Haz preguntas especÃ­ficas.  
    - Evita PDFs escaneados (sin texto seleccionable).
    """)


ke = st.text_input("ğŸ”‘ Ingresa tu clave de OpenAI:", type="password")

if ke:
    os.environ['OPENAI_API_KEY'] = ke
else:
    st.info("Por favor ingresa tu clave de API para continuar.")


pdf = st.file_uploader("ğŸ“ Carga un archivo PDF", type="pdf")

if pdf is not None and ke:
    try:
        st.markdown("### ğŸ§¾ Extrayendo texto del PDF...")
        pdf_reader = PdfReader(pdf)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text()

        st.success(f"âœ… Texto extraÃ­do correctamente ({len(text)} caracteres).")

        # DivisiÃ³n del texto
        text_splitter = CharacterTextSplitter(
            separator="\n",
            chunk_size=500,
            chunk_overlap=20,
            length_function=len
        )
        chunks = text_splitter.split_text(text)
        st.info(f"ğŸ“‘ Documento dividido en {len(chunks)} fragmentos.")

        # Embeddings + Base de conocimiento
        with st.spinner("ğŸ” Generando embeddings y construyendo base de conocimiento..."):
            embeddings = OpenAIEmbeddings()
            knowledge_base = FAISS.from_texts(chunks, embeddings)

        # Interfaz de preguntas
        st.markdown("### ğŸ’¬ Haz una pregunta sobre tu documento:")
        user_question = st.text_area("âœï¸ Escribe tu pregunta aquÃ­...", placeholder="Ejemplo: Â¿CuÃ¡l es el objetivo principal del documento?")

        if user_question:
            with st.spinner("ğŸ¤” Analizando tu pregunta..."):
                docs = knowledge_base.similarity_search(user_question)

                # Modelo actualizado
                llm = OpenAI(temperature=0, model_name="gpt-4o")
                chain = load_qa_chain(llm, chain_type="stuff")

                # Obtener respuesta
                response = chain.run(input_documents=docs, question=user_question)

            # Mostrar respuesta
            st.markdown("### ğŸ¯ Respuesta:")
            st.success(response)

    except Exception as e:
        st.error(f"âŒ Error al procesar el PDF: {str(e)}")
        import traceback
        st.error(traceback.format_exc())

elif pdf is not None and not ke:
    st.warning("âš ï¸ Ingresa tu clave de API antes de continuar.")
else:
    st.info("ğŸ“‚ Carga un archivo PDF para comenzar el anÃ¡lisis.")


st.markdown("---")
st.caption("Hecho con â¤ï¸ usando Streamlit, LangChain y OpenAI.")
